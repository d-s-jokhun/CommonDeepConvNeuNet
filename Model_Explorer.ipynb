{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from DataPartition import DataPartition\n",
    "from im_import import Import_GrayImg\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "import multiprocessing as mp\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import operator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from mdl_Ch_adjuster import mdl_Ch_adjuster\n",
    "import os\n",
    "import sys\n",
    "from RegularizeModel import RegularizeModel\n",
    "from SaveModelDescript import SaveModelDescript\n",
    "from ModelEditor import ModelEditor\n",
    "from get_CompileParams import get_CompileParams\n",
    "from callback_ConfMat import callback_ConfMat\n",
    "import random\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get fashion_mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "get_fashion_mnist = False\n",
    "\n",
    "if get_fashion_mnist:\n",
    "#     MasterPath = os.path.abspath(\"/gpfs0/home/jokhun/fashion_mnist/\")\n",
    "    MasterPath = os.path.abspath('//deptnas.nus.edu.sg/BIE/MBELab/jokhun/fashion_mnist/')   \n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    x_val = x_test ; y_val = y_test  \n",
    "    \n",
    "    idx=np.random.choice(range(0,len(x_train)), size=1000, replace=False)\n",
    "    x_train=np.array([x_train[n] for n in idx])\n",
    "    y_train=np.array([y_train[n] for n in idx])\n",
    "    idx=np.random.choice(range(0,len(x_val)), size=750, replace=False)\n",
    "    x_val=np.array([x_val[n] for n in idx])\n",
    "    y_val=np.array([y_val[n] for n in idx])\n",
    "    idx=np.random.choice(range(0,len(x_test)), size=500, replace=False)\n",
    "    x_test=np.array([x_test[n] for n in idx])\n",
    "    y_test=np.array([y_test[n] for n in idx])    \n",
    "    \n",
    "    X_Train = tf.expand_dims(x_train, axis=-1)\n",
    "    X_Val = tf.expand_dims(x_val, axis=-1)\n",
    "    X_Test = tf.expand_dims(x_test, axis=-1)\n",
    "    print('X_Train shape:'+str(X_Train.shape) + '   X_Val shape:' + str(X_Val.shape) + '   X_Test shape:' + str(X_Test.shape))\n",
    "\n",
    "    ResponseEncoder = LabelEncoder()\n",
    "    ResponseEncoder.fit(list(y_train) + list(y_val) + list(y_test))\n",
    "    class_names = ResponseEncoder.classes_\n",
    "    NumOfClasses = len(class_names)\n",
    "    print('Number of calsses in the data: '+str(NumOfClasses))\n",
    "    print('Classes in the Data: ' + str(class_names))\n",
    "    Y_Train = ResponseEncoder.transform(y_train)\n",
    "    Y_Val = ResponseEncoder.transform(y_val)\n",
    "    Y_Test = ResponseEncoder.transform(y_test)\n",
    "    print ('1st element of Tr_Y, Val_Y and Ts_Y : ' + str(y_train[0]) + ', ' + str(y_val[0]) + ', ' + str(y_test[0]))\n",
    "    print ('1st element of Y_Train, Y_Val and Y_Test : ' + str(Y_Train[0]) + ', ' + str(Y_Val[0]) + ', ' + str(Y_Test[0]))\n",
    "    \n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    print('Actual Classes in the Data: ' + str(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the input X and Y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes available :\n",
      " ['0_DMSO_Complete', 'DMSO', 'caffeine', 'chlorphenamine', 'estradiol', 'paracetamol']\n"
     ]
    }
   ],
   "source": [
    "# MasterPath = os.path.abspath(\"/gpfs0/home/jokhun/\")\n",
    "MasterPath = os.path.abspath('//deptnas.nus.edu.sg/BIE/MBELab/jokhun/Pro 1/U2OS small mol screening')\n",
    "\n",
    "Segmented_MasterFolder = 'Segmented_SmallMol'\n",
    "\n",
    "Classes = sorted([Class for Class in os.listdir(os.path.join(MasterPath,Segmented_MasterFolder)) \n",
    "           if os.path.isdir(os.path.join(MasterPath,Segmented_MasterFolder,Class))])\n",
    "print('Classes available :\\n',Classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the following cell to select specific classes rather than all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes selected :\n",
      " ['DMSO', 'caffeine', 'chlorphenamine', 'estradiol', 'paracetamol']\n",
      "No. of Images available =  21996\n",
      "estradiol  :  1982\n",
      "caffeine  :  3104\n",
      "paracetamol  :  3443\n",
      "DMSO  :  6000\n",
      "chlorphenamine  :  7467\n"
     ]
    }
   ],
   "source": [
    "Select_Classes = True # Set to False in order to select all available classes\n",
    "selection = range(1,len(Classes))#[1,4] # List of classes to be selected. Only used if Select_Classes is True\n",
    "\n",
    "if Select_Classes:\n",
    "    Selected_Classes = list(operator.itemgetter(*selection)(Classes))\n",
    "else:\n",
    "    Selected_Classes = Classes\n",
    "\n",
    "ClassPaths={}\n",
    "for Class in Selected_Classes:\n",
    "    ClassPaths[Class]=sorted(glob.glob(os.path.join(\n",
    "        MasterPath,Segmented_MasterFolder,Class,f\"*_{Class}.tif\"\n",
    "    )))\n",
    "\n",
    "print('Classes selected :\\n',Selected_Classes)\n",
    "print('No. of Images available = ', np.sum([len(ClassPaths[Class]) for Class in ClassPaths]))\n",
    "sorted_dict = {k: v for k, v in sorted(ClassPaths.items(), key=lambda item: len(item[1]))}\n",
    "for Class in sorted_dict: print (Class,' : ',len(ClassPaths[Class]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capping class size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. of paths: 320\n",
      "Min Class size: 64 ['DMSO', 'caffeine', 'chlorphenamine', 'estradiol', 'paracetamol']\n",
      "Max Class size: 64 ['DMSO', 'caffeine', 'chlorphenamine', 'estradiol', 'paracetamol']\n",
      "\n",
      "Class sizes (ascending): [64, 64, 64, 64, 64]\n",
      "DMSO  :  64\n",
      "caffeine  :  64\n",
      "chlorphenamine  :  64\n",
      "estradiol  :  64\n",
      "paracetamol  :  64\n"
     ]
    }
   ],
   "source": [
    "# Restricting the number of images per class\n",
    "cap_class_size = True\n",
    "\n",
    "MaxClassSize = 64 #np.amin([len(items[1]) for items in ClassPaths.items()])\n",
    "np.random.seed(None)\n",
    "\n",
    "if cap_class_size:\n",
    "    for Class in ClassPaths.keys():\n",
    "        ClassPaths[Class]=sorted(np.random.choice(ClassPaths[Class], \n",
    "                                                  size = MaxClassSize, replace = True))\n",
    "else:\n",
    "    for Class in ClassPaths.keys():\n",
    "        ClassPaths[Class]=sorted(ClassPaths[Class])\n",
    "\n",
    "Ascd_ClassSizes = sorted([len(items[1]) for items in ClassPaths.items()])\n",
    "print('Total No. of paths:', np.sum(Ascd_ClassSizes))\n",
    "Max_Classes = [Class for Class in ClassPaths.keys() if len(ClassPaths[Class])==Ascd_ClassSizes[0]]\n",
    "print('Min Class size:',Ascd_ClassSizes[0], Max_Classes)\n",
    "Min_Classes = [Class for Class in ClassPaths.keys() if len(ClassPaths[Class])==Ascd_ClassSizes[-1]]\n",
    "print('Max Class size:',Ascd_ClassSizes[-1], Min_Classes) \n",
    "print('\\nClass sizes (ascending):',Ascd_ClassSizes) \n",
    "\n",
    "sorted_dict = {k: v for k, v in sorted(ClassPaths.items(), key=lambda item: len(item[1]))}\n",
    "for Class in sorted_dict: print (Class,' : ',len(ClassPaths[Class]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitioning data X and creating labels Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of paths = 320\n",
      "\n",
      "Length of Training Set = 250\n",
      "Classes in Training Set : ['DMSO' 'caffeine' 'chlorphenamine' 'estradiol' 'paracetamol']\n",
      " ------- Frequencies : [50 50 50 50 50]\n",
      " Distinct Frequencies : [50, 50, 50, 50, 50]\n",
      "\n",
      "Length of Validation Set = 65\n",
      "Classes in Training Set : ['DMSO' 'caffeine' 'chlorphenamine' 'estradiol' 'paracetamol']\n",
      " ------- Frequencies : [13 13 13 13 13]\n",
      " Distinct Frequencies : 13\n",
      "\n",
      "Length of Test Set = 5\n",
      "Classes in Training Set : ['DMSO' 'caffeine' 'chlorphenamine' 'estradiol' 'paracetamol']\n",
      " ------- Frequencies : [1 1 1 1 1]\n",
      " Distinct Frequencies : 1\n",
      "\n",
      "1st element of Training Set : caffeine\n",
      "\\\\deptnas.nus.edu.sg\\BIE\\MBELab\\jokhun\\Pro 1\\U2OS small mol screening\\Segmented_SmallMol\\caffeine\\24306_c09_s1_40_caffeine.tif\n",
      "1st element of Validation Set : paracetamol\n",
      "\\\\deptnas.nus.edu.sg\\BIE\\MBELab\\jokhun\\Pro 1\\U2OS small mol screening\\Segmented_SmallMol\\paracetamol\\25986_a12_s4_5_paracetamol.tif\n",
      "1st element of Test Set : DMSO\n",
      "\\\\deptnas.nus.edu.sg\\BIE\\MBELab\\jokhun\\Pro 1\\U2OS small mol screening\\Segmented_SmallMol\\DMSO\\24306_g20_s1_25_DMSO.tif\n"
     ]
    }
   ],
   "source": [
    "# Y can be determined either from the filenames or the folders from which the images are loaded\n",
    "\n",
    "get_labels_from = 'folders' # 'Filenames' or 'Folders'\n",
    "RanSeed=None\n",
    "Partition=[0.79,0.2,0.01]\n",
    "\n",
    "PartitionedPaths = DataPartition(ClassPaths, Partition=Partition, RanSeed=RanSeed)\n",
    "\n",
    "random.seed(RanSeed)\n",
    "Tr_Paths=[]; Val_Paths=[]; Ts_Paths=[];\n",
    "for key in PartitionedPaths.keys():\n",
    "    Tr_Set,Val_Set,Ts_Set=PartitionedPaths[key]['Tr_Set'],PartitionedPaths[key]['Val_Set'],PartitionedPaths[key]['Ts_Set']\n",
    "    Tr_Paths.extend(Tr_Set), Val_Paths.extend(Val_Set), Ts_Paths.extend(Ts_Set)\n",
    "random.shuffle(Tr_Paths); random.shuffle(Val_Paths); random.shuffle(Ts_Paths);\n",
    "Tr_Paths=np.array(Tr_Paths); Val_Paths=np.array(Val_Paths); Ts_Paths=np.array(Ts_Paths);\n",
    "\n",
    "if get_labels_from.lower() == 'filenames':\n",
    "    Tr_Y = [path[path.rindex('_') + 1 : path.index('.tif')] for path in Tr_Paths]\n",
    "    Val_Y = [path[path.rindex('_') + 1 : path.index('.tif')] for path in Val_Paths]\n",
    "    Ts_Y = [path[path.rindex('_') + 1 : path.index('.tif')] for path in Ts_Paths]\n",
    "\n",
    "elif get_labels_from.lower() == 'folders':\n",
    "    Tr_Y = [os.path.basename(os.path.dirname(path)) for path in Tr_Paths]\n",
    "    Val_Y = [os.path.basename(os.path.dirname(path)) for path in Val_Paths]\n",
    "    Ts_Y = [os.path.basename(os.path.dirname(path)) for path in Ts_Paths]\n",
    "    \n",
    "else: sys.exit(\"Invalid entry for 'get_labels_from'!\")\n",
    "\n",
    "print ('Total number of paths = ' + str(len(Tr_Paths)+len(Val_Paths)+len(Ts_Paths)))\n",
    "print ('\\nLength of Training Set = ' + str(len(Tr_Paths)))\n",
    "values, counts = np.unique(Tr_Y, return_counts=True)\n",
    "Dis_counts = [len(ClassPaths[Class])-(round(Ascd_ClassSizes[0]*Partition[2])+round(Ascd_ClassSizes[0]*Partition[1])) for Class in ClassPaths.keys()]\n",
    "print ('Classes in Training Set : ' + str(values) + '\\n ------- Frequencies : ' + str(counts) + '\\n Distinct Frequencies : ' + str(Dis_counts))\n",
    "print ('\\nLength of Validation Set = ' + str(len(Val_Paths)))\n",
    "values, counts = np.unique(Val_Y, return_counts=True)\n",
    "Dis_counts = round(Ascd_ClassSizes[0]*Partition[1])\n",
    "print ('Classes in Training Set : ' + str(values) + '\\n ------- Frequencies : ' + str(counts) + '\\n Distinct Frequencies : ' + str(Dis_counts))\n",
    "print ('\\nLength of Test Set = ' + str(len(Ts_Paths)))\n",
    "values, counts = np.unique(Ts_Y, return_counts=True)\n",
    "Dis_counts = round(Ascd_ClassSizes[0]*Partition[2])\n",
    "print ('Classes in Training Set : ' + str(values) + '\\n ------- Frequencies : ' + str(counts) + '\\n Distinct Frequencies : ' + str(Dis_counts))\n",
    "\n",
    "print (f'\\n1st element of Training Set : {Tr_Y[0]}\\n' + str(Tr_Paths[0]))\n",
    "print (f'1st element of Validation Set : {Val_Y[0]}\\n' + str(Val_Paths[0]))\n",
    "print (f'1st element of Test Set : {Ts_Y[0]}\\n' + str(Ts_Paths[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import images: Tr_X, Val_X and Ts_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed during import = 2.420215279000004 s\n",
      "Length of Training Set = 250\n",
      "Length of Validation Set = 65\n",
      "Length of Test Set = 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Text(0.5, 1.0, 'Test[0]'), <matplotlib.image.AxesImage at 0x1d635f2e1c8>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ImgScaleFactor = 1\n",
    "DesiredImgSize = 64\n",
    "\n",
    "start=time.perf_counter()\n",
    "with mp.Pool() as pool:\n",
    "    Tr_X = pool.starmap(Import_GrayImg, [(path,ImgScaleFactor,DesiredImgSize) for path in Tr_Paths])\n",
    "    Val_X = pool.starmap(Import_GrayImg, [(path,ImgScaleFactor,DesiredImgSize) for path in Val_Paths])\n",
    "    Ts_X = pool.starmap(Import_GrayImg, [(path,ImgScaleFactor,DesiredImgSize) for path in Ts_Paths])\n",
    "print('Time elapsed during import = '+ str(time.perf_counter() - start) + ' s')\n",
    "\n",
    "print ('Length of Training Set = '+str(len(Tr_X)))\n",
    "print ('Length of Validation Set = '+str(len(Val_X)))\n",
    "print ('Length of Test Set = '+str(len(Ts_X)))\n",
    "\n",
    "plt.subplot(1,3,1).set_title('Train[0]'), plt.imshow(Tr_X[0], cmap='gray', norm=matplotlib.colors.Normalize())\n",
    "plt.subplot(1,3,2).set_title('Val[0]'), plt.imshow(Val_X[0], cmap='gray', norm=matplotlib.colors.Normalize())\n",
    "plt.subplot(1,3,3).set_title('Test[0]'), plt.imshow(Ts_X[0], cmap='gray', norm=matplotlib.colors.Normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of invalid files = 1\n",
      "Classes in Training Set : ['DMSO' 'caffeine' 'chlorphenamine' 'estradiol' 'paracetamol'] --- Frequencies : [50 50 50 50 50]\n",
      "Classes in Validation Set : ['DMSO' 'caffeine' 'chlorphenamine' 'estradiol' 'paracetamol'] --- Frequencies : [13 13 13 12 13]\n",
      "Classes in Test Set : ['DMSO' 'caffeine' 'chlorphenamine' 'estradiol' 'paracetamol'] --- Frequencies : [1 1 1 1 1]\n",
      "\n",
      "Invalid Traininig files = 0\n",
      "[]\n",
      "\n",
      "Invalid Val files = 1\n",
      "['\\\\\\\\deptnas.nus.edu.sg\\\\BIE\\\\MBELab\\\\jokhun\\\\Pro 1\\\\U2OS small mol screening\\\\Segmented_SmallMol\\\\estradiol\\\\24295_i02_s1_2_estradiol.tif']\n",
      "\n",
      "Invalid Test files = 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Getting rid of invalid images (if th enucleus was too large to fit within 'DesiredImgSize')\n",
    "Invalid_Tr = [i for i,val in enumerate(Tr_X) if type(val)==type(None)]\n",
    "for idx in sorted(Invalid_Tr, reverse=True):\n",
    "    del Tr_X[idx]\n",
    "    del Tr_Y[idx]\n",
    "\n",
    "Invalid_Val = [i for i,val in enumerate(Val_X) if type(val)==type(None)]\n",
    "for idx in sorted(Invalid_Val, reverse=True):\n",
    "    del Val_X[idx]\n",
    "    del Val_Y[idx]\n",
    "\n",
    "Invalid_Ts = [i for i,val in enumerate(Ts_X) if type(val)==type(None)]\n",
    "for idx in sorted(Invalid_Ts, reverse=True):\n",
    "    del Ts_X[idx]\n",
    "    del Ts_Y[idx]\n",
    "\n",
    "print ('Total number of invalid files = '+str(len(Invalid_Tr)+len(Invalid_Val)+len(Invalid_Ts)))\n",
    "values, counts = np.unique(Tr_Y, return_counts=True)\n",
    "print ('Classes in Training Set : ' + str(values) + ' --- Frequencies : ' + str(counts))\n",
    "values, counts = np.unique(Val_Y, return_counts=True)\n",
    "print ('Classes in Validation Set : ' + str(values) + ' --- Frequencies : ' + str(counts))\n",
    "values, counts = np.unique(Ts_Y, return_counts=True)\n",
    "print ('Classes in Test Set : ' + str(values) + ' --- Frequencies : ' + str(counts))\n",
    "\n",
    "print('\\nInvalid Traininig files = '+str(len(Invalid_Tr))+'\\n'+str(operator.itemgetter(Invalid_Tr)(Tr_Paths)))\n",
    "print('\\nInvalid Val files = '+str(len(Invalid_Val))+'\\n'+str(operator.itemgetter(Invalid_Val)(Val_Paths)))\n",
    "print('\\nInvalid Test files = '+str(len(Invalid_Ts))+'\\n'+str(operator.itemgetter(Invalid_Ts)(Ts_Paths)))\n",
    "Tr_Paths = np.delete(Tr_Paths,Invalid_Tr)\n",
    "Val_Paths = np.delete(Val_Paths,Invalid_Val)\n",
    "Ts_Paths = np.delete(Ts_Paths,Invalid_Ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restructuring the image dataset and encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train shape:(250, 64, 64, 1)   X_Val shape:(64, 64, 64, 1)   X_Test shape:(5, 64, 64, 1)\n",
      "Number of calsses in the data: 5\n",
      "Classes in the Data: ['DMSO' 'caffeine' 'chlorphenamine' 'estradiol' 'paracetamol']\n",
      "1st element of Tr_Y, Val_Y and Ts_Y : caffeine, paracetamol, DMSO\n",
      "1st element of Y_Train, Y_Val and Y_Test : 1, 4, 0\n"
     ]
    }
   ],
   "source": [
    "X_Train = tf.expand_dims(Tr_X, axis=-1)\n",
    "X_Val = tf.expand_dims(Val_X, axis=-1)\n",
    "X_Test = tf.expand_dims(Ts_X, axis=-1)\n",
    "print('X_Train shape:'+str(X_Train.shape) + '   X_Val shape:' + str(X_Val.shape) + '   X_Test shape:' + str(X_Test.shape))\n",
    "\n",
    "ResponseEncoder = LabelEncoder()\n",
    "ResponseEncoder.fit(list(Tr_Y) + list(Val_Y) + list(Ts_Y))\n",
    "class_names = ResponseEncoder.classes_\n",
    "NumOfClasses = len(class_names)\n",
    "print('Number of calsses in the data: '+str(NumOfClasses))\n",
    "print('Classes in the Data: ' + str(class_names))\n",
    "Y_Train = ResponseEncoder.transform(Tr_Y)\n",
    "Y_Val = ResponseEncoder.transform(Val_Y)\n",
    "Y_Test = ResponseEncoder.transform(Ts_Y)\n",
    "print ('1st element of Tr_Y, Val_Y and Ts_Y : ' + str(Tr_Y[0]) + ', ' + str(Val_Y[0]) + ', ' + str(Ts_Y[0]))\n",
    "print ('1st element of Y_Train, Y_Val and Y_Test : ' + str(Y_Train[0]) + ', ' + str(Y_Val[0]) + ', ' + str(Y_Test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the models ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating keras models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  model/s loaded!\n"
     ]
    }
   ],
   "source": [
    "UseExistingArchitectureCores = True\n",
    "\n",
    "models = {}\n",
    "if UseExistingArchitectureCores:\n",
    "    models['try_Xception'] = tf.keras.applications.Xception(include_top=False, weights=\"imagenet\", input_shape=(299, 299, 3))\n",
    "#     models['try_InceptionV3'] = tf.keras.applications.InceptionV3(include_top=False, weights=\"imagenet\", input_shape=(299, 299, 3))\n",
    "#     models['try_InceptionResNetV2'] = tf.keras.applications.InceptionResNetV2(include_top=False, weights=\"imagenet\", input_shape=(299, 299, 3))\n",
    "#     models['try_ResNet50V2'] = tf.keras.applications.ResNet50V2(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "#     models['try_DenseNet201'] = tf.keras.applications.DenseNet201(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "#     models['try_NASNetLarge'] = tf.keras.applications.NASNetLarge(include_top=False, weights=\"imagenet\", input_shape=(331, 331, 3))\n",
    "          \n",
    "    ModelKeys=list(models.keys())\n",
    "    ModelsCreated = len(ModelKeys)\n",
    "    print (str(ModelsCreated),' model/s loaded!')\n",
    "else:\n",
    "    print ('No model created. Load one from disk below!')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "SaveModelDescription = False\n",
    "\n",
    "if SaveModelDescription:\n",
    "    for ModelKey in ModelKeys:\n",
    "        model = models[ModelKey]\n",
    "        Model_Path = os.path.join(MasterPath,str(ModelKey))        \n",
    "        SaveModelDescript(model, save_dir=Model_Path, \n",
    "                          save_filename=str(ModelKey))        \n",
    "    print ('Model descriptions saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Edit_Model = False\n",
    "SaveEditedModelDescription = True\n",
    "ModelKey = ModelKeys[0]\n",
    "\n",
    "# if Edit_Model:\n",
    "#     model = models[ModelKey]\n",
    "#     New_Layers={'drop1':tf.keras.layers.Dropout(rate=0.1, name='drop1'),\n",
    "#                 'drop2':tf.keras.layers.Dropout(rate=0.2, name='drop2'),\n",
    "#                 'drop3':tf.keras.layers.Dropout(rate=0.3, name='drop3'),\n",
    "#                 'drop4':tf.keras.layers.Dropout(rate=0.4, name='drop4'),\n",
    "#                 'drop5':tf.keras.layers.Dropout(rate=0.5, name='drop5'),\n",
    "#                }\n",
    "\n",
    "#     IncomingLinks_2Axe=[-18, -12, -14, -8, -5, -1]   \n",
    "\n",
    "#     IncomingLinks_2Forge=[(New_Layers['drop1'], model.layers[-19]),\n",
    "#                           (model.layers[-18], New_Layers['drop1']),\n",
    "#                           (model.layers[-12], New_Layers['drop1']),\n",
    "#                           (New_Layers['drop2'], model.layers[-15]),\n",
    "#                           (model.layers[-14], New_Layers['drop2']),\n",
    "#                           (New_Layers['drop3'], model.layers[-9]),\n",
    "#                           (model.layers[-8], New_Layers['drop3']),\n",
    "#                           (New_Layers['drop4'], model.layers[-6]),\n",
    "#                           (model.layers[-5], New_Layers['drop4']),\n",
    "#                           (New_Layers['drop5'], model.layers[-2]),\n",
    "#                           (model.layers[-1], New_Layers['drop5']),\n",
    "#                          ]\n",
    "\n",
    "#     model_inputs=None\n",
    "#     model_outputs=None\n",
    "\n",
    "#     model = ModelEditor(model, New_Layers=New_Layers, IncomingLinks_2Axe=IncomingLinks_2Axe, \n",
    "#                                 IncomingLinks_2Forge=IncomingLinks_2Forge,\n",
    "#                                 model_inputs=model_inputs, model_outputs=model_outputs)\n",
    "#     models[ModelKey] = model \n",
    "    \n",
    "#     # Save edited model description\n",
    "#     if SaveEditedModelDescription:\n",
    "#         Model_Path = os.path.join(MasterPath,str(ModelKey))        \n",
    "#         SaveModelDescript(model, save_dir=Model_Path, \n",
    "#                           save_filename=str(ModelKey+'_edited'))        \n",
    "#         print ('Model descriptions saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Top and Bottom layers to keras models instantiated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom and Top layers added!\n"
     ]
    }
   ],
   "source": [
    "AddTopAndBottomLayers = True\n",
    "\n",
    "if AddTopAndBottomLayers:\n",
    "    ModelKeys=list(models.keys())\n",
    "    for ModelKey in ModelKeys:\n",
    "        core = models[ModelKey]\n",
    "        core.trainable = False\n",
    "        \n",
    "        In = tf.keras.Input(shape=(X_Train.shape[1:4]), name=\"Preprocess_Input\")\n",
    "        Ch_adjuster = mdl_Ch_adjuster(Input_ImgSize=X_Train.shape[1:4], Output_ImgSize=core.input_shape[1:4])\n",
    "        preprocess_layers = eval('tf.keras.applications.'+core.name+'.preprocess_input')\n",
    "        Out = preprocess_layers(Ch_adjuster(In))\n",
    "        mdl_preprocess = tf.keras.Model(inputs=In, outputs=Out, name='mdl_preprocess')\n",
    "        \n",
    "        In = tf.keras.Input(shape=(core.output_shape[1:4]), name=\"Features\")\n",
    "        GlbAvgPool = tf.keras.layers.GlobalAveragePooling2D(name=\"GlbAvgPool\")\n",
    "        GlbMaxPool = tf.keras.layers.GlobalMaxPooling2D(name=\"GlbMaxPool\")\n",
    "        Concatenate = tf.keras.layers.Concatenate(name=\"AvgMax_Concat\")\n",
    "        Dropout = tf.keras.layers.Dropout(0.2, name=\"Feature_Dropout\")\n",
    "        predictions = tf.keras.layers.Dense(units=NumOfClasses, activation=None, name=\"predictions\")\n",
    "        Out=predictions(Dropout(Concatenate([GlbAvgPool(In),GlbMaxPool(In)])))\n",
    "        mdl_prediction = tf.keras.Model(inputs=In, outputs=Out, name='mdl_prediction')\n",
    "\n",
    "        \n",
    "        In = tf.keras.Input(shape=(X_Train.shape[1:4]), name=\"Input_images\")\n",
    "        Out = mdl_prediction(core(mdl_preprocess(In)))\n",
    "        models[ModelKey] = tf.keras.Model(inputs=In, outputs=Out, name=ModelKey)\n",
    "    print ('Bottom and Top layers added!')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model descriptions saved!\n"
     ]
    }
   ],
   "source": [
    "SaveModelDescription = True\n",
    "\n",
    "if SaveModelDescription:\n",
    "    ModelKeys=list(models.keys())\n",
    "    for ModelKey in ModelKeys:\n",
    "        model = models[ModelKey]\n",
    "        Model_Path = os.path.join(MasterPath,str(ModelKey))        \n",
    "        SaveModelDescript(model, save_dir=Model_Path, \n",
    "                          save_filename=str(ModelKey))        \n",
    "    print ('Model descriptions saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models compiled!\n"
     ]
    }
   ],
   "source": [
    "CompileModels = True\n",
    "\n",
    "if CompileModels:\n",
    "    ModelKeys = list(models.keys())\n",
    "    for ModelKey in ModelKeys:\n",
    "        model = models[ModelKey]\n",
    "        optimizer,loss,metrics,loss_weights,weighted_metrics,run_eagerly = get_CompileParams (model).values()\n",
    "        if optimizer!=None:\n",
    "            model.compile(optimizer = optimizer,\n",
    "                          loss = loss,\n",
    "                          metrics = metrics,\n",
    "                          loss_weights = loss_weights,\n",
    "                          weighted_metrics = weighted_metrics,\n",
    "                          run_eagerly = run_eagerly)\n",
    "        else:\n",
    "            model.compile(optimizer='adam', \n",
    "                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "                          metrics=['accuracy'])\n",
    "    print ('Models compiled!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading models from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "LoadModelFromDisk = False\n",
    "\n",
    "# if LoadModelFromDisk:\n",
    "#     models['mdl_name'] = tf.keras.models.load_model('mdl_path')\n",
    "    \n",
    "#     ModelsLoaded = len(models.keys()) - ModelsCreated\n",
    "#     print (str(ModelsLoaded),' models loaded from disk!')\n",
    "# else:\n",
    "#     print ('No model loaded from disk!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Regularization to all regularizable layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "RegularizeTheModel = False\n",
    "# if RegularizeTheModel:\n",
    "#     regularizer = tf.keras.regularizers.l1_l2(l1=0, l2=0.001)\n",
    "#     for ModelKey in ModelKeys:\n",
    "#         models[ModelKey]=RegularizeModel(models[ModelKey], regularizer, keep_weights=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of models =  1\n",
      "Initial Train Loss and Accuracy\n",
      "try_Xception : [2.525848611831665, 0.204]\n",
      "\n",
      "Initial Val Loss and Accuracy\n",
      "try_Xception : [2.5172410011291504, 0.203125]\n",
      "\n",
      "Initial Test Loss and Accuracy\n",
      "try_Xception : [2.541525363922119, 0.2]\n"
     ]
    }
   ],
   "source": [
    "ModelKeys=list(models.keys())\n",
    "\n",
    "print ('Total number of models = ',str(len(models.keys())))\n",
    "print ('Initial Train Loss and Accuracy')\n",
    "TrainEval=[]\n",
    "for ModelKey in ModelKeys:\n",
    "    Eval=models[ModelKey].evaluate(X_Train,Y_Train, verbose=0)\n",
    "    TrainEval.append(str(ModelKey)+' : '+str(Eval))\n",
    "print ('\\n'.join(TrainEval)) \n",
    "\n",
    "print ('\\nInitial Val Loss and Accuracy')\n",
    "ValEval=[]\n",
    "for ModelKey in ModelKeys:\n",
    "    Eval=models[ModelKey].evaluate(X_Val,Y_Val, verbose=0)\n",
    "    ValEval.append(str(ModelKey)+' : '+str(Eval))\n",
    "print ('\\n'.join(ValEval)) \n",
    "\n",
    "print ('\\nInitial Test Loss and Accuracy')\n",
    "TestEval=[]\n",
    "for ModelKey in ModelKeys:\n",
    "    Eval=models[ModelKey].evaluate(X_Test,Y_Test, verbose=0)\n",
    "    TestEval.append(str(ModelKey)+' : '+str(Eval))\n",
    "print ('\\n'.join(TestEval)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "UseImageDataGenerator = True\n",
    "\n",
    "def ImgGrayscale (img):\n",
    "    bw = img>0\n",
    "    img = np.subtract(img, np.amin(img))\n",
    "    img = np.divide(img, np.amax(img))\n",
    "    img = img*bw   \n",
    "    return img\n",
    "\n",
    "if UseImageDataGenerator:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        zca_epsilon=1e-06,\n",
    "        rotation_range=180,\n",
    "        width_shift_range=0.0,\n",
    "        height_shift_range=0.0,\n",
    "        brightness_range=None,\n",
    "        shear_range=0.0,\n",
    "        zoom_range=0.0,\n",
    "        channel_shift_range=0.0,\n",
    "        fill_mode=\"constant\",\n",
    "        cval=0,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rescale=None,\n",
    "        preprocessing_function=ImgGrayscale,\n",
    "        data_format=None,\n",
    "        validation_split=0.0,\n",
    "        dtype=None,\n",
    "    )\n",
    "else:\n",
    "    datagen = ImageDataGenerator(preprocessing_function=ImgGrayscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model (model, X_Train=None, Y_Train=None, X_Val=None, Y_Val=None, batch_size=64, initial_epoch=0, final_epoch=5, Model_Path=None, class_names=None, shuffle=True, DatagenShuffleSeed=None):\n",
    "    if Model_Path==None or Model_Path==[]:\n",
    "        Model_Path=model.name     \n",
    "        \n",
    "        \n",
    "    sess_DateTime = str(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    MdlChkpt_Path = os.path.join(Model_Path,\"MdlChkpt\",(sess_DateTime+\"_e{epoch:03d}_Acc{accuracy:.2f}_ValAcc{val_accuracy:.2f}.ckpt\"))\n",
    "    MdlChkpt_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        MdlChkpt_Path, monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, \n",
    "        mode='auto', save_freq=\"epoch\"\n",
    "    )\n",
    "    TensorBoard_Path = os.path.join(Model_Path,\"logs\",(model.name+'_'+sess_DateTime))\n",
    "    TensorBoard_cb = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = TensorBoard_Path, histogram_freq=0, write_graph=False, write_images=False, update_freq=\"epoch\", \n",
    "        profile_batch=0, embeddings_freq=0, embeddings_metadata=None\n",
    "    )\n",
    "    \n",
    "    ConfMat_Path = os.path.join(Model_Path,\"logs\",(model.name+'_'+sess_DateTime))\n",
    "    log_confusion_matrix=callback_ConfMat(model=model, X=X_Val, Y=Y_Val, class_names=class_names, logdir=ConfMat_Path)\n",
    "    # Define the per-epoch callback.\n",
    "    ConfMat_cb = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
    "    \n",
    "    \n",
    "    history = model.fit(\n",
    "        datagen.flow(x=X_Train, y=Y_Train, batch_size=batch_size, \n",
    "                     shuffle=shuffle, sample_weight=None,seed=DatagenShuffleSeed,\n",
    "                     save_to_dir=None,save_prefix=\"\",save_format=\"png\",subset=None),\n",
    "        initial_epoch=initial_epoch, epochs=final_epoch, steps_per_epoch=len(X_Train)/batch_size, \n",
    "        verbose=1, callbacks=[MdlChkpt_cb, TensorBoard_cb, ConfMat_cb], \n",
    "        validation_data=(X_Val,Y_Val), shuffle=shuffle, use_multiprocessing=False\n",
    "    )        \n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreshTrainHistory = True\n",
    "if FreshTrainHistory:\n",
    "    history={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training non-core layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the core model as non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainNonCoreOnly = True\n",
    "CoreModel_layer = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TrainNonCoreOnly:\n",
    "    ModelKeys=list(models.keys())\n",
    "    for ModelKey in ModelKeys:\n",
    "        model=models[ModelKey]\n",
    "        model.layers[CoreModel_layer].trainable = False\n",
    "        model.compile(optimizer='adam', \n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "                      metrics=['accuracy'])\n",
    "        print (model.layers[CoreModel_layer].name,'(core model) has been set as non-trainable and', ModelKey, 'recompiled!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"try_Xception\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_images (InputLayer)    [(None, 64, 64, 1)]       0         \n",
      "_________________________________________________________________\n",
      "mdl_preprocess (Model)       (None, 299, 299, 3)       3         \n",
      "_________________________________________________________________\n",
      "xception (Model)             (None, 10, 10, 2048)      20861480  \n",
      "_________________________________________________________________\n",
      "mdl_prediction (Model)       (None, 5)                 20485     \n",
      "=================================================================\n",
      "Total params: 20,881,968\n",
      "Trainable params: 20,488\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if TrainNonCoreOnly:\n",
    "    ModelKeys=list(models.keys())\n",
    "    for ModelKey in ModelKeys:\n",
    "        print (models[ModelKey].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training non-core layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training try_Xception Non-Core...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 7.8125 steps, validate on 64 samples\n",
      "Epoch 1/3\n",
      "8/7 [==============================] - 21s 3s/step - loss: 2.7548 - accuracy: 0.1960 - val_loss: 1.9804 - val_accuracy: 0.1875\n",
      "Epoch 2/3\n",
      "8/7 [==============================] - 12s 2s/step - loss: 2.2397 - accuracy: 0.2400 - val_loss: 2.2034 - val_accuracy: 0.2500\n",
      "Epoch 3/3\n",
      "8/7 [==============================] - 12s 2s/step - loss: 2.4149 - accuracy: 0.1840 - val_loss: 2.2212 - val_accuracy: 0.2656\n",
      "\n",
      "try_Xception Non-Core trained! Training time = 0.7615234698666673 min!\n",
      "\n",
      "Total training time = 0.012692105274722237 hr!\n"
     ]
    }
   ],
   "source": [
    "if TrainNonCoreOnly:\n",
    "    Start=time.perf_counter()\n",
    "    ModelKeys=list(models.keys())\n",
    "    for ModelKey in ModelKeys:\n",
    "        ModelStart=time.perf_counter()\n",
    "        print('\\nTraining '+str(ModelKey)+' Non-Core...')\n",
    "        Model_Path = os.path.join(MasterPath,str(ModelKey))\n",
    "        model = models[ModelKey]\n",
    "        sess_DateTime = str(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        history[ModelKey+'_'+sess_DateTime+'_NonCore']=train_model (\n",
    "            model, X_Train=X_Train, Y_Train=Y_Train, X_Val=X_Val, Y_Val=Y_Val, \n",
    "            batch_size=32, initial_epoch=0, final_epoch=3,\n",
    "            Model_Path=Model_Path, class_names=class_names, shuffle=True, DatagenShuffleSeed=None)\n",
    "        \n",
    "        print('\\n'+str(ModelKey)+' Non-Core trained! Training time = '+ str((time.perf_counter()-ModelStart)/60) + ' min!')\n",
    "    print('\\nTotal training time = '+ str((time.perf_counter()-Start)/(60*60)) + ' hr!')        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of models =  1\n",
      "Train Loss and Accuracy\n",
      "try_Xception : [2.2191745109558108, 0.196]\n",
      "\n",
      "Val Loss and Accuracy\n",
      "try_Xception : [2.2211719751358032, 0.265625]\n",
      "\n",
      "Test Loss and Accuracy\n",
      "try_Xception : [2.3998160362243652, 0.2]\n"
     ]
    }
   ],
   "source": [
    "print ('Total number of models = ',str(len(models.keys())))\n",
    "print ('Train Loss and Accuracy')\n",
    "TrainEval=[]\n",
    "ModelKeys=list(models.keys())\n",
    "for ModelKey in ModelKeys:\n",
    "    Eval=models[ModelKey].evaluate(X_Train,Y_Train, verbose=0)\n",
    "    TrainEval.append(str(ModelKey)+' : '+str(Eval))\n",
    "print ('\\n'.join(TrainEval)) \n",
    "\n",
    "print ('\\nVal Loss and Accuracy')\n",
    "ValEval=[]\n",
    "for ModelKey in ModelKeys:\n",
    "    Eval=models[ModelKey].evaluate(X_Val,Y_Val, verbose=0)\n",
    "    ValEval.append(str(ModelKey)+' : '+str(Eval))\n",
    "print ('\\n'.join(ValEval)) \n",
    "\n",
    "print ('\\nTest Loss and Accuracy')\n",
    "TestEval=[]\n",
    "for ModelKey in ModelKeys:\n",
    "    Eval=models[ModelKey].evaluate(X_Test,Y_Test, verbose=0)\n",
    "    TestEval.append(str(ModelKey)+' : '+str(Eval))\n",
    "print ('\\n'.join(TestEval)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine training additional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting some layers of the core model as trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "FineTrainCoreLayers = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoreModel_layer = CoreModel_layer\n",
    "\n",
    "ModelKeys=list(models.keys())\n",
    "\n",
    "FineTuneOnwards = {ModelKeys[0]:-7,\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appropriate layers after -7 of xception have been set as trainable!\n",
      "try_Xception has been recompiled!\n"
     ]
    }
   ],
   "source": [
    "if FineTrainCoreLayers:\n",
    "    ModelKeys=list(models.keys())\n",
    "    for ModelKey in ModelKeys:\n",
    "        core_model = models[ModelKey].layers[CoreModel_layer]\n",
    "        core_model.trainable = True\n",
    "        for layer in core_model.layers[:FineTuneOnwards[ModelKey]]:\n",
    "            layer.trainable =  False\n",
    "        print ('Appropriate layers after',FineTuneOnwards[ModelKey],'of',core_model.name,'have been set as trainable!')\n",
    "        models[ModelKey].compile(optimizer=tf.keras.optimizers.Adam(1e-5), \n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "                      metrics=['accuracy'])\n",
    "        print (models[ModelKey].name,'has been recompiled!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"try_Xception\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_images (InputLayer)    [(None, 64, 64, 1)]       0         \n",
      "_________________________________________________________________\n",
      "mdl_preprocess (Model)       (None, 299, 299, 3)       3         \n",
      "_________________________________________________________________\n",
      "xception (Model)             (None, 10, 10, 2048)      20861480  \n",
      "_________________________________________________________________\n",
      "mdl_prediction (Model)       (None, 5)                 20485     \n",
      "=================================================================\n",
      "Total params: 20,881,968\n",
      "Trainable params: 4,769,288\n",
      "Non-trainable params: 16,112,680\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if FineTrainCoreLayers:\n",
    "    ModelKeys=list(models.keys())\n",
    "    for ModelKey in ModelKeys:\n",
    "        print (models[ModelKey].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training try_Xception...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 7.8125 steps, validate on 64 samples\n",
      "Epoch 4/5\n",
      "8/7 [==============================] - 29s 4s/step - loss: 2.3728 - accuracy: 0.2480 - val_loss: 2.1655 - val_accuracy: 0.2344\n",
      "Epoch 5/5\n",
      "8/7 [==============================] - 35s 4s/step - loss: 2.2070 - accuracy: 0.2160 - val_loss: 2.1225 - val_accuracy: 0.2500\n",
      "\n",
      "try_Xception trained! Training time = 1.068879763616667 min!\n",
      "\n",
      "Total training time = 0.017814711365555555 hr!\n"
     ]
    }
   ],
   "source": [
    "if FineTrainCoreLayers:\n",
    "    Start=time.perf_counter()\n",
    "    ModelKeys=list(models.keys())\n",
    "    for ModelKey in ModelKeys:\n",
    "        ModelStart=time.perf_counter()\n",
    "        print('\\nTraining '+str(ModelKey)+'...')\n",
    "        Model_Path = os.path.join(MasterPath,str(ModelKey))\n",
    "        model = models[ModelKey]\n",
    "        sess_DateTime = str(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        history[ModelKey+'_'+sess_DateTime+'_FineTune']=train_model (\n",
    "            model, X_Train=X_Train, Y_Train=Y_Train, X_Val=X_Val, Y_Val=Y_Val, \n",
    "            batch_size=32, initial_epoch=3, final_epoch=5,\n",
    "            Model_Path=Model_Path, class_names=class_names, shuffle=True, DatagenShuffleSeed=None)\n",
    "        \n",
    "        print('\\n'+str(ModelKey)+' trained! Training time = '+ str((time.perf_counter()-ModelStart)/60) + ' min!')\n",
    "    print('\\nTotal training time = '+ str((time.perf_counter()-Start)/(60*60)) + ' hr!')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of models =  1\n",
      "Train Loss and Accuracy\n",
      "try_Xception : [2.129371652603149, 0.204]\n",
      "\n",
      "Val Loss and Accuracy\n",
      "try_Xception : [2.1224586963653564, 0.25]\n",
      "\n",
      "Test Loss and Accuracy\n",
      "try_Xception : [2.3612780570983887, 0.2]\n"
     ]
    }
   ],
   "source": [
    "print ('Total number of models = ',str(len(models.keys())))\n",
    "print ('Train Loss and Accuracy')\n",
    "TrainEval=[]\n",
    "ModelKeys=list(models.keys())\n",
    "for ModelKey in ModelKeys:\n",
    "    Eval=models[ModelKey].evaluate(X_Train,Y_Train, verbose=0)\n",
    "    TrainEval.append(str(ModelKey)+' : '+str(Eval))\n",
    "print ('\\n'.join(TrainEval)) \n",
    "\n",
    "print ('\\nVal Loss and Accuracy')\n",
    "ValEval=[]\n",
    "for ModelKey in ModelKeys:\n",
    "    Eval=models[ModelKey].evaluate(X_Val,Y_Val, verbose=0)\n",
    "    ValEval.append(str(ModelKey)+' : '+str(Eval))\n",
    "print ('\\n'.join(ValEval)) \n",
    "\n",
    "print ('\\nTest Loss and Accuracy')\n",
    "TestEval=[]\n",
    "for ModelKey in ModelKeys:\n",
    "    Eval=models[ModelKey].evaluate(X_Test,Y_Test, verbose=0)\n",
    "    TestEval.append(str(ModelKey)+' : '+str(Eval))\n",
    "print ('\\n'.join(TestEval)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the latest version of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveLatestVersions = True\n",
    "if SaveLatestVersions:\n",
    "    sess = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    for ModelKey in ModelKeys:\n",
    "        print('\\nSaving '+str(ModelKey))\n",
    "        Save_Path = os.path.join(MasterPath,str(ModelKey),(\"LastModel_\"+sess))\n",
    "        models[ModelKey].save(\n",
    "            Save_Path, overwrite=False, include_optimizer=True, save_format=None,\n",
    "            signatures=None, options=None\n",
    "        )\n",
    "    print('\\nThe latest version of each model has been saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
